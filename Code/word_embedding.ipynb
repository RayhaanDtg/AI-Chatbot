{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["rom doc2vec import Doc2VecModel,create_corpus,intents"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from utils import generate_embed_matrix\n", "from utils import encode_pad_corpus\n", "from keras.models import Sequential\n", "from keras.layers.core import Dense , Flatten ,Dropout\n", "from keras.layers import Embedding,LSTM,Bidirectional,BatchNormalization\n", "#from keras.models import Model\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.model_selection import train_test_split\n", "import tensorflow as tf\n", "import  keras.callbacks as callbacks\n", "#import ModelCheckpoint\n", "# get the doc to vec model to retrieve the vocabulary size of the corpus\n", "# generate the training data\n", "# build the embedding model with keras library\n", "# train the model on the docs\n", "# save the model to retrieve the embedding matrix later"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["       \n", "        \n", "def build_LSTM(vocab_size,output,embed_matrix):\n", "    model = Sequential()\n", "    model.add(Embedding(input_dim=vocab_size,output_dim=output, trainable=False,input_length=32,weights=[embed_matrix]))\n", "    model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1, dropout=0.1), 'concat'))\n", "    model.add(Dropout(0.1))\n", "    model.add(LSTM(256, return_sequences=False, recurrent_dropout=0.1, dropout=0.1))    \n", "    model.add(Dropout(0.1))\n", "    #model.add(Dense(512, activation='relu',kernel_regularizer ='l2'))\n", "    model.add(Dense(512, activation='relu',kernel_regularizer ='l2'))\n\n", "    #model.add(Flatten())\n", "    model.add(Dense(11, activation='softmax'))\n", "    \n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_data,Y_data,t,le=encode_pad_corpus()\n", "print('X_data is: ', X_data)\n", "embed_mat=generate_embed_matrix(t)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_val, y_train, y_val = train_test_split(X_data, Y_data, test_size = 0.3, \n", "                                                   shuffle = True, stratify=Y_data, random_state = 7)\n", "vocab_size=len(t.word_counts) +1\n", "print('Vocab size is: ', vocab_size)\n", "lstm_model=build_LSTM(vocab_size,100,embed_mat)\n", "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n", "lstm_model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nitializing checkpoint settings to view progress and save model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["filename = 'Models/Bi-LSTM_2.h5'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Learning rate scheduling<br>\n", "This function keeps the initial learning rate for the first ten epochs  <br>\n", "and decreases it exponentially after that.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def scheduler(epoch, learn_rate):\n", "    if epoch < 20:\n", "        return learn_rate\n", "    else:\n", "        return learn_rate * tf.math.exp(-0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sched_checkpoint = callbacks.LearningRateScheduler(scheduler)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This saves the best model<br>\n", "checkpoint = callbacks.ModelCheckpoint(filename,  verbose=1, <br>\n", "                             save_best_only=True, mode='min')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fitting model with all the callbacks above"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hist = lstm_model.fit(X_train, y_train, epochs = 50, batch_size = 32, \n", "                 validation_data = (X_val, y_val), \n", "                 callbacks = [sched_checkpoint])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lstm_model.save(filename)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}