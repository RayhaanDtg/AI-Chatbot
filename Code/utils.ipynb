{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nltk\n", "from nltk import WordNetLemmatizer\n", "from intent_cluster import generate_training_data\n", "from gensim.models.doc2vec import Doc2Vec\n", "import numpy as np\n", "from keras.preprocessing.text import one_hot\n", "from keras.preprocessing.sequence import pad_sequences\n", "import pandas as pd\n", "from nltk.corpus.reader import wordnet\n", "#from keras.models import Model\n", "from sklearn.preprocessing import LabelEncoder\n", "from keras.preprocessing.text import Tokenizer\n", "from nltk.tokenize import TweetTokenizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t=Tokenizer()\n", "model=Doc2Vec.load(\"Models/Doc2Vec.model\")\n", "training_df=generate_training_data()\n", "vocab_size=len(model.wv)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_len=32\n", "def generate_embed_matrix(tkz):\n", "    print('In the embedding function: ')\n", "    embeddings_index={}\n", "    f = open('models/glove.twitter.27B.100d.txt',encoding=\"utf8\")\n", "    for line in f:\n", "        values = line.split()\n", "        word = values[0]\n", "        coefs = np.asarray(values[1:], dtype='float32')\n", "       \n", "        embeddings_index[word] = coefs\n", "    f.close()\n", "    word_index = tkz.word_index\n", "    EMBEDDING_DIM = 100 # Because we are using the 100D gloVe embeddings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Getting the embedding matrix of shape (size of word_index, embedd dimension)<br>\n", "Hence, each word index will be embedded in a 100D"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n", "    for word, i in word_index.items():\n", "        embedding_vector = embeddings_index.get(word)\n", "        if embedding_vector is not None:\n", "            embedding_matrix[i] = embedding_vector\n", "    print(embedding_matrix)\n", "    return embedding_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def encode_pad_corpus():\n", "    le=LabelEncoder()\n", "    coded_df=pd.DataFrame(columns=['Intents','Docs'])\n", "    \n", "    for i in range(len(training_df['Documents'])):\n", "        for j in range(len(training_df['Documents'][i])):\n", "            coded_df=coded_df.append({'Intents':training_df['Intents'][i],'Docs':training_df['Documents'][i][j]},ignore_index=True)\n", "           \n", "    le.fit(coded_df['Intents'])\n", "    y_data=le.transform(coded_df['Intents'])\n", "    t.fit_on_texts(coded_df['Docs'])\n", "    X_data=t.texts_to_sequences(coded_df['Docs'])\n", "    x_data_pad=pad_sequences(X_data, maxlen = max_len, padding = 'post')\n", "    print(t.word_index['macbook'])\n", "    #print(t.word_index['would'])\n", "    return x_data_pad,y_data,t,le"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lemmatize(phrase):\n", "    tknzr=TweetTokenizer(strip_handles=True, reduce_len=True)\n", "    lemmatizer=WordNetLemmatizer()\n", "    lst=tknzr.tokenize(phrase)\n", "    lst=nltk.pos_tag([word.lower() for word in phrase])\n", "    print(lst)\n", "        \n", "    lst= [lemmatizer.lemmatize(word[0],get_pos_tag(word[1])) for word in lst if get_pos_tag(word[1]) is not None]\n", "    print(lst)\n", "       \n", "    return lst"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_pos_tag(tag):\n", "        \n", "        if tag in ['JJ', 'JJR', 'JJS']:\n", "            return wordnet.ADJ\n", "        elif tag in ['RB', 'RBR', 'RBS']:\n", "            return wordnet.ADV\n", "        elif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n", "            return wordnet.NOUN\n", "        elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n", "            return wordnet.VERB\n", "        return None "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}