{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nltk\n", "from nltk.corpus.reader import wordnet\n", "# nltk.download('stopwords')\n", "# nltk.download('punkt')\n", "#nltk.download('averaged_perceptron_tagger')\n", "nltk.download('wordnet')\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from nltk.corpus import stopwords\n", "from spacy import displacy\n", "from nltk.tokenize import TweetTokenizer\n", "from nltk.stem import  WordNetLemmatizer\n", "import re"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stop_words = set(stopwords.words(\"english\"))\n", "class Preprocess_Pipeline:\n\n", "    # class that loads the json file dataset\n", "    # it has its own lemmatizer and stemmer objects \n", "    # it uses a Tweet Tokenizer to tokenize the json dataset \n", "    # since the Tweet Tokenizer removes words like @\n", "    def __init__(self,filename):\n", "        #f=open(filename)\n", "        #self.data=json.load(f)\n", "        self.data=pd.read_pickle(filename)\n", "        self.lemmatizer=WordNetLemmatizer()\n", "        \n", "        self.tokenized_word=[]\n", "        self.tknzr=TweetTokenizer(strip_handles=True, reduce_len=True)\n", "        self.final_lst=[]\n\n", "    # tokenizes all the sentences in the answer/questions of the dataset and \n", "    # # remove all the punctuations from the tokenized list of words\n", "    # remove the links from the questions\n", "    # remove all the stop words from the tokenized questions\n", "    # lemmatize each word in the question\n", "    def tokenize_clean_data(self):\n", "        self.data['question']=self.data.apply(lambda row:re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', row['question'], flags=re.MULTILINE), axis=1)\n", "        self.data['question']=self.data.apply(lambda row:self.tknzr.tokenize(row['question']), axis=1)\n", "        self.data['question']=self.data.apply(lambda row:[word for word in row['question'] if word.isalnum()], axis=1)\n", "        self.data['question']=self.data.apply(lambda row:[word for word in row['question']if word not in stop_words],axis=1)\n", "        self.data['question']=self.data.apply(lambda row: self.lemmatize_phrase(row['question']),axis=1)\n", "        self.data['title']=self.data.apply(lambda row:self.tknzr.tokenize(row['title']), axis=1)\n", "       \n", "        self.data['title']=self.data.apply(lambda row:[word for word in row['title'] if word.isalnum()], axis=1)\n", "        self.data['title']=self.data.apply(lambda row:[word for word in row['title']if word not in stop_words],axis=1)\n", "        self.data['title']=self.data.apply(lambda row: self.lemmatize_phrase(row['title']),axis=1)\n", "      "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}